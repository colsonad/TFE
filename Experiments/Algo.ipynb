{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modele import Modele\n",
    "from Optimiseur import Optimiseur\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a simple dataset\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Plot the dataset\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modele(Lambda,gamma,X_train):\n",
    "    # Create a model and optimizer with the given parameters\n",
    "    # Initialize the initial vector randomly\n",
    "    modele = Modele(Lambda,gamma)\n",
    "    optimiseur = Optimiseur(modele)\n",
    "    L,mu = modele.constante_L(X_train)\n",
    "    init= np.random.randn(X_train.shape[0]) \n",
    "    return modele, optimiseur, L, mu, init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model, optimizer\n",
    "modele, optimiseur, L, mu, init = create_modele(0,30,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def figure(stepsizes,L,train_loss,norm,name1,name2,GD=False):\n",
    "    \"\"\"\n",
    "    Function to plot the stepsizes, train function, and norm of the gradient.\n",
    "    Parameters:\n",
    "    - stepsizes: List of stepsizes used in the optimization.\n",
    "    - L: Lipschitz constant.\n",
    "    - train_loss: List of training function values through the iterations.\n",
    "    - norm: List of norms of the gradient values through the iterations.\n",
    "    - name1: Filename for the stepsizes plot.\n",
    "    - name2: Filename for the train function and norm plot.\n",
    "    - GD: Boolean indicating if the optimization is gradient descent.\n",
    "    \"\"\"\n",
    "    if not GD:\n",
    "        # Plot the stepsizes evolution except if GD\n",
    "        stepsizes = np.array(stepsizes)\n",
    "        plt.figure()\n",
    "        plt.plot(stepsizes * L, label='Stepsizes')\n",
    "        plt.hlines(y=2, xmin=0, xmax=len(stepsizes), colors='r', linestyles='dashed', label='2')\n",
    "        plt.xlabel('Iteration', fontsize=14)\n",
    "        plt.ylabel('Stepsize * L', fontsize=14)\n",
    "        plt.title('Stepsizes over Iterations', fontsize=16)\n",
    "        plt.grid()\n",
    "        plt.savefig(name1)\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "    # Plot train function on the first subplot\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    axs[0].plot(train_loss, label='Train Loss', color='blue')\n",
    "    axs[0].set_xlabel('Iteration', fontsize=14)\n",
    "    axs[0].set_ylabel('Train Function', fontsize=14)\n",
    "    axs[0].set_title('Train Function over Iterations', fontsize=16)\n",
    "    axs[0].grid()\n",
    "\n",
    "    # Plot norm on the second subplot\n",
    "    axs[1].plot(norm, label='Norm', color='orange')\n",
    "    axs[1].set_xlabel('Iteration', fontsize=14)\n",
    "    axs[1].set_ylabel('Norm of the Gradient', fontsize=14)\n",
    "    axs[1].set_title('Norm of the Gradient over Iterations', fontsize=16)\n",
    "    axs[1].grid()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(name2)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and plot figures using gradient descent with different step sizes\n",
    "\n",
    "# step size = 1/L\n",
    "alpha, alpha_list = optimiseur.gradient_descent(X_train, y_train, init.copy(),1/L, 500, 0.001, \"norm\")\n",
    "train_loss, test_loss, norm, train_accuracy, test_accuracy = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "figure(None,L,train_loss,norm,None,\"GD_train_1.pdf\",True)\n",
    "\n",
    "# step size = 2/L\n",
    "alpha, alpha_list = optimiseur.gradient_descent(X_train, y_train, init.copy(),2/L, 500, 0.001, \"norm\")\n",
    "train_loss, test_loss, norm, train_accuracy, test_accuracy = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "figure(None,L,train_loss,norm,None,\"GD_train_2.pdf\",True)\n",
    "\n",
    "# step size = 2.1/L\n",
    "alpha, alpha_list = optimiseur.gradient_descent(X_train, y_train, init.copy(),2.1/L, 500, 0.001, \"norm\")\n",
    "train_loss, test_loss, norm, train_accuracy, test_accuracy = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "figure(None,L,train_loss,norm,None,\"GD_train_21.pdf\",True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model and plot figures using exact line serach towards error to optimal step sizes\n",
    "alpha, alpha_list,stepsizes = optimiseur.optimal_stepsize(X_train, y_train, init.copy(), 1000, 0.001, \"norm\",True)\n",
    "train_loss, test_loss, norm, train_accuracy, test_accuracy = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "figure(stepsizes,L,train_loss,norm,\"optimal_stepsize.pdf\",\"optimal_train.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model and plot figures using exact line serach towards error vanishing gradient\n",
    "alpha, alpha_list,stepsizes = optimiseur.exact_stepsize(X_train, y_train, init.copy(), 1000, 0.001, \"norm\",True)\n",
    "train_loss, test_loss, norm, train_accuracy, test_accuracy = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "figure(stepsizes,L,train_loss,norm,\"exact_stepsize.pdf\",\"exact_train.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model and plot figures using dynamical step size\n",
    "alpha, alpha_list,stepsizes = optimiseur.dynamic_stepsize(X_train, y_train, init.copy(), 1000, 0.001, \"norm\",True)\n",
    "train_loss, test_loss, norm, train_accuracy, test_accuracy = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "figure(stepsizes,L,train_loss,norm,\"dynamic_stepsize.pdf\",\"dynamic_train.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and plot figures using periodically large step sizes for different number of iterations\n",
    "\n",
    "# stop when converging at 0.001\n",
    "alpha, alpha_list,stepsizes = optimiseur.Periodic(X_train, y_train, init.copy(), 1000, 0.001, \"norm\",True)\n",
    "train_loss, test_loss, norm, train_accuracy, test_accuracy = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "figure(stepsizes,L,train_loss,norm,\"periodic_stepsize.pdf\",\"periodic_train.pdf\")\n",
    "\n",
    "# stop after 127 iterations\n",
    "alpha, alpha_list,stepsizes = optimiseur.Periodic(X_train, y_train, init.copy(), 127, 0.000001, \"norm\",True)\n",
    "train_loss, test_loss, norm, train_accuracy, test_accuracy = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "figure(stepsizes,L,train_loss,norm,\"periodic_stepsize_127.pdf\",\"periodic_train_127.pdf\")\n",
    "\n",
    "# stop after 500 iterations\n",
    "alpha, alpha_list,stepsizes = optimiseur.Periodic(X_train, y_train, init.copy(), 500, 0.000001, \"norm\",True)\n",
    "train_loss, test_loss, norm, train_accuracy, test_accuracy = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "figure(stepsizes,L,train_loss,norm,\"periodic_stepsize_500.pdf\",\"periodic_train_500.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
