{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modele import Modele\n",
    "from Optimiseur import Optimiseur\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_svmlight_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CKN_MNIST(n,q):\n",
    "    # Load the CKN MNIST dataset\n",
    "    \n",
    "    data = np.load('ckn_mnist.npz')\n",
    "\n",
    "    # Access the arrays in the dataset\n",
    "    images = data['X']\n",
    "    labels = data['y']\n",
    "    #  Binarization of the labels\n",
    "    labels = np.where(labels < 5, -1, 1)\n",
    "\n",
    "    indices = np.random.choice(images.shape[0], n, replace=False)\n",
    "    images_ = images[indices]\n",
    "    labels_ = labels[indices]\n",
    "    # Separe the dataset into training and testing\n",
    "    n_train = int(n*q)\n",
    "    train_images = images_[:n_train]\n",
    "    train_labels = labels_[:n_train]\n",
    "    test_images = images_[n_train:]\n",
    "    test_labels = labels_[n_train:]\n",
    "    # Reshape the images to 2D\n",
    "    X_train = train_images.reshape(train_images.shape[0], -1)\n",
    "    X_test = test_images.reshape(test_images.shape[0], -1)\n",
    "    y_train = train_labels.reshape(-1)\n",
    "    y_test = test_labels.reshape(-1)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a4a(): \n",
    "    # Load the a4a dataset\n",
    "    \n",
    "    X_train_, y_train = load_svmlight_file(\"a4a.txt\")\n",
    "    X_test_, y_test = load_svmlight_file(\"a4a_t.Txt\")   \n",
    "    X_test_ = X_test_[:,:-1]\n",
    "    X_train_dense = X_train_.toarray()  \n",
    "    X_test_dense = X_test_.toarray()\n",
    "\n",
    "    X_train = X_train_dense.reshape(-1, 122) \n",
    "    X_test = X_test_dense.reshape(-1, 122)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_modele(Lambda,gamma,X_train):\n",
    "    # Create the model and optimizer, and initialize randomly the initial vector\n",
    "    modele = Modele(Lambda,gamma)\n",
    "    optimiseur = Optimiseur(modele)\n",
    "    L,mu = modele.constante_L(X_train)\n",
    "    init= np.random.randn(X_train.shape[0]) \n",
    "    return modele, optimiseur, L, mu, init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd_mean(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,name):\n",
    "    \"\"\" Perform gradient descent with different learning rates between 1.9/L and 1.99/L and compute the mean of the results\"\"\"\n",
    "    \n",
    "    learning_rates = [1.9/L,1.91/L,1.92/L,1.93/L,1.94/L,1.95/L,1.96/L,1.97/L,1.98/L,1.99/L]\n",
    "\n",
    "    # Initialize lists to store results for each learning rate\n",
    "    train_loss_gd_= []\n",
    "    test_loss_gd_= []\n",
    "    norm_gd_= []\n",
    "    train_accuracy_gd_= []\n",
    "    test_accuracy_gd_= []\n",
    "    # Iterate over each learning rate and perform gradient descent\n",
    "    for lr in learning_rates:\n",
    "        alpha, alpha_list = optimiseur.gradient_descent(X_train, y_train, init.copy(),lr, max_iters, target, criterion)\n",
    "        train_loss_list, test_loss_list, norm_list, train_accuracy_list, test_accuracy_list = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "\n",
    "        train_loss_gd_.append(train_loss_list)\n",
    "        test_loss_gd_.append(test_loss_list)\n",
    "        norm_gd_.append(norm_list)\n",
    "        train_accuracy_gd_.append(train_accuracy_list)\n",
    "        test_accuracy_gd_.append(test_accuracy_list)\n",
    "\n",
    "    # Find the minimum length among all arrays in train_loss_iteration_99\n",
    "    max_length = max(len(arr) for arr in train_loss_gd_)\n",
    "\n",
    "    # Create copies of the lists to avoid modifying the original lists\n",
    "    train_loss_gd= train_loss_gd_.copy()\n",
    "    test_loss_gd= test_loss_gd_.copy()\n",
    "    norm_gd= norm_gd_.copy()\n",
    "    train_accuracy_gd= train_accuracy_gd_.copy()\n",
    "    test_accuracy_gd= test_accuracy_gd_.copy()\n",
    "\n",
    "    # Truncate or pad each array to the maximum length\n",
    "    for i in range(len(train_loss_gd_)):\n",
    "        if len(train_loss_gd[i]) < max_length:\n",
    "            train_loss_gd[i] = np.append(train_loss_gd[i], [train_loss_gd[i][-1]] * (max_length - len(train_loss_gd[i])))\n",
    "            test_loss_gd[i] = np.append(test_loss_gd[i], [test_loss_gd[i][-1]] * (max_length - len(test_loss_gd[i])))\n",
    "            norm_gd[i] = np.append(norm_gd[i], [norm_gd[i][-1]] * (max_length - len(norm_gd[i])))\n",
    "            train_accuracy_gd[i] = np.append(train_accuracy_gd[i], [train_accuracy_gd[i][-1]] * (max_length - len(train_accuracy_gd[i])))\n",
    "            test_accuracy_gd[i] = np.append(test_accuracy_gd[i], [test_accuracy_gd[i][-1]] * (max_length - len(test_accuracy_gd[i])))\n",
    "    # Compute the mean across the truncated arrays\n",
    "\n",
    "    #save into .npy files\n",
    "    np.save(name+\"train_loss.npy\", train_loss_gd)\n",
    "    np.save(name+\"test_loss.npy\", test_loss_gd)\n",
    "    np.save(name+\"norm.npy\", norm_gd)\n",
    "    np.save(name+\"train_accuracy.npy\", train_accuracy_gd)\n",
    "    np.save(name+\"test_accuracy.npy\", test_accuracy_gd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,name):\n",
    "    \"\"\" Perform gradient descent with different learning rates and save the results\"\"\"\n",
    "\n",
    "    learning_rates=[1/L,1.5/L,1.9/L]\n",
    "    # Initialize lists to store results for each learning rate\n",
    "    train_loss_iteration= []\n",
    "    test_loss_iteration= []\n",
    "    norm_iteration= []\n",
    "    train_accuracy_iteration= []\n",
    "    test_accuracy_iteration= []\n",
    "    # Iterate over each learning rate and perform gradient descent\n",
    "    for lr in learning_rates:\n",
    "        alpha, alpha_list = optimiseur.gradient_descent(X_train, y_train, init.copy(),lr, max_iters, target, criterion)\n",
    "        train_loss_list, test_loss_list, norm_list, train_accuracy_list, test_accuracy_list = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "\n",
    "        train_loss_iteration.append(train_loss_list)\n",
    "        test_loss_iteration.append(test_loss_list)\n",
    "        norm_iteration.append(norm_list)\n",
    "        train_accuracy_iteration.append(train_accuracy_list)\n",
    "        test_accuracy_iteration.append(test_accuracy_list)\n",
    "\n",
    "    # Save the results into .npy files\n",
    "    np.save(name+\"train_loss_1.npy\", np.array(train_loss_iteration[0]))\n",
    "    np.save(name+\"test_loss_1.npy\", np.array(test_loss_iteration[0]))\n",
    "    np.save(name+\"norm_1.npy\", np.array(norm_iteration[0]))\n",
    "    np.save(name+\"train_accuracy_1.npy\", np.array(train_accuracy_iteration[0]))\n",
    "    np.save(name+\"test_accuracy_1.npy\", np.array(test_accuracy_iteration[0]))\n",
    "    np.save(name+\"train_loss_15.npy\", np.array(train_loss_iteration[1]))\n",
    "    np.save(name+\"test_loss_15.npy\", np.array(test_loss_iteration[1]))\n",
    "    np.save(name+\"norm_15.npy\", np.array(norm_iteration[1]))\n",
    "    np.save(name+\"train_accuracy_15.npy\", np.array(train_accuracy_iteration[1]))\n",
    "    np.save(name+\"test_accuracy_15.npy\", np.array(test_accuracy_iteration[1]))\n",
    "    np.save(name+\"train_loss_2.npy\", np.array(train_loss_iteration[2]))\n",
    "    np.save(name+\"test_loss_2.npy\", np.array(test_loss_iteration[2]))\n",
    "    np.save(name+\"norm_2.npy\", np.array(norm_iteration[2]))\n",
    "    np.save(name+\"train_accuracy_2.npy\", np.array(train_accuracy_iteration[2]))\n",
    "    np.save(name+\"test_accuracy_2.npy\", np.array(test_accuracy_iteration[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,name):\n",
    "    \"\"\" Perform gradient descent with dynamic stepsize and save the results\"\"\"\n",
    "    alpha, alpha_list,stepsizes_dynamic = optimiseur.dynamic_stepsize(X_train, y_train, init.copy(), max_iters, target, criterion,True)\n",
    "    train_loss_dynamic, test_loss_dynamic, norm_dynamic, train_accuracy_dynamic, test_accuracy_dynamic = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "    \n",
    "    np.save(name+\"train_loss.npy\", train_loss_dynamic)\n",
    "    np.save(name+\"test_loss.npy\", test_loss_dynamic)\n",
    "    np.save(name+\"norm.npy\", norm_dynamic)\n",
    "    np.save(name+\"train_accuracy.npy\", train_accuracy_dynamic)\n",
    "    np.save(name+\"test_accuracy.npy\", test_accuracy_dynamic)\n",
    "    np.save(name+\"stepsizes.npy\", stepsizes_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,name):\n",
    "    \"\"\" Perform gradient descent with optimal stepsize and save the results\"\"\"\n",
    "    alpha, alpha_list,stepsizes = optimiseur.optimal_stepsize(X_train, y_train, init.copy(), max_iters, target, criterion,True)\n",
    "    train_loss, test_loss, norm, train_accuracy, test_accuracy = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "    np.save(name+\"train_loss.npy\", train_loss)\n",
    "    np.save(name+\"test_loss.npy\", test_loss)\n",
    "    np.save(name+\"norm.npy\", norm)\n",
    "    np.save(name+\"train_accuracy.npy\", train_accuracy)\n",
    "    np.save(name+\"test_accuracy.npy\", test_accuracy)\n",
    "    np.save(name+\"stepsizes.npy\", stepsizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,name):\n",
    "    \"\"\" Perform gradient descent with exact stepsize and save the results\"\"\"\n",
    "    alpha, alpha_list,stepsizes = optimiseur.exact_stepsize(X_train, y_train, init.copy(), max_iters, target, criterion,True)\n",
    "    train_loss, test_loss, norm, train_accuracy, test_accuracy = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "    np.save(name+\"train_loss.npy\", train_loss)\n",
    "    np.save(name+\"test_loss.npy\", test_loss)\n",
    "    np.save(name+\"norm.npy\", norm)\n",
    "    np.save(name+\"train_accuracy.npy\", train_accuracy)\n",
    "    np.save(name+\"test_accuracy.npy\", test_accuracy)\n",
    "    np.save(name+\"stepsizes.npy\", stepsizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def periodic(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,name):\n",
    "    \"\"\" Perform gradient descent with periodic stepsize and save the results\"\"\"\n",
    "    alpha, alpha_list,stepsizes = optimiseur.Periodic(X_train, y_train, init.copy(), max_iters, target, criterion,True)\n",
    "    train_loss, test_loss, norm, train_accuracy, test_accuracy = modele.compute_all(X_train,X_test, y_train, y_test, alpha_list)\n",
    "    np.save(name+\"train_loss.npy\", train_loss)\n",
    "    np.save(name+\"test_loss.npy\", test_loss)\n",
    "    np.save(name+\"norm.npy\", norm)\n",
    "    np.save(name+\"train_accuracy.npy\", train_accuracy)\n",
    "    np.save(name+\"test_accuracy.npy\", test_accuracy)\n",
    "    np.save(name+\"stepsizes.npy\", stepsizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the two datasets\n",
    "X_train, y_train, X_test, y_test = CKN_MNIST(10000,0.8)\n",
    "X_train_a4a, y_train_a4a, X_test_a4a, y_test_a4a = a4a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and optimizer for CKN MNIST dataset\n",
    "modele, optimiseur, L, mu, init = create_modele(0,30,X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model and optimizer for a4a dataset\n",
    "modele_a4a, optimiseur_a4a, L_a4a, mu_a4a, init_a4a = create_modele(0,0.6,X_train_a4a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17393520948644733\n",
      "0.10023564216320321\n",
      "(np.float64(0.17393520948644733), np.float64(1.2777231223710166e-07))\n"
     ]
    }
   ],
   "source": [
    "print(L)\n",
    "print(L_a4a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameters for the optimization\n",
    "max_iters=5000\n",
    "target=1e-2\n",
    "criterion='norm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent with different large constant learning rates for a4a dataset\n",
    "gd_mean(optimiseur_a4a,modele_a4a,X_train_a4a,y_train_a4a,X_test_a4a,y_test_a4a,init_a4a,L_a4a,max_iters,criterion,target,\"gd_mean_001_a4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent with different constant learning rates for a4a dataset\n",
    "gd(optimiseur_a4a,modele_a4a,X_train_a4a,y_train_a4a,X_test_a4a,y_test_a4a,init_a4a,L_a4a,max_iters,criterion,target,\"gd_001_a4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent with dynamic stepsize for a4a dataset\n",
    "dynamic(optimiseur_a4a,modele_a4a,X_train_a4a,y_train_a4a,X_test_a4a,y_test_a4a,init_a4a,L_a4a,max_iters,criterion,target,\"dynamic_001_a4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent with optimal stepsize for a4a dataset\n",
    "optimal(optimiseur_a4a,modele_a4a,X_train_a4a,y_train_a4a,X_test_a4a,y_test_a4a,init_a4a,L_a4a,max_iters,criterion,target,\"optimal_001_a4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent with exact stepsize for a4a dataset\n",
    "exact(optimiseur_a4a,modele_a4a,X_train_a4a,y_train_a4a,X_test_a4a,y_test_a4a,init_a4a,L_a4a,max_iters,criterion,target,\"exact_001_a4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent with periodic stepsize for a4a dataset\n",
    "periodic(optimiseur_a4a,modele_a4a,X_train_a4a,y_train_a4a,X_test_a4a,y_test_a4a,init_a4a,L_a4a,max_iters,criterion,target,\"periodic_001_a4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent with different large constant learning rates for CKN-MNIST dataset\n",
    "gd_mean(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,\"gd_mean_001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent with different constant learning rates for CKN-MNIST dataset\n",
    "gd(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,\"gd_001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent with dynamic stepsize for CKN-MNIST dataset\n",
    "dynamic(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,\"dynamic_001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent with optimal stepsize for CKN-MNIST dataset\n",
    "optimal(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,\"optimal_001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent with exact stepsize for CKN-MNIST dataset\n",
    "exact(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,\"exact_001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform gradient descent with periodic stepsize for CKN-MNIST dataset\n",
    "periodic(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,\"periodic_001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for other stopping criterion\n",
    "max_iters=5000\n",
    "target=5e-3\n",
    "criterion='norm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_mean(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,\"gd_mean_0005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,\"gd_0005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,\"dynamic_0005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,\"optimal_0005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,\"exact_0005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodic(optimiseur,modele,X_train,y_train,X_test,y_test,init,L,max_iters,criterion,target,\"periodic_0005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_mean(optimiseur_a4a,modele_a4a,X_train_a4a,y_train_a4a,X_test_a4a,y_test_a4a,init_a4a,L_a4a,max_iters,criterion,target,\"gd_mean_0005_a4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd(optimiseur_a4a,modele_a4a,X_train_a4a,y_train_a4a,X_test_a4a,y_test_a4a,init_a4a,L_a4a,max_iters,criterion,target,\"gd_0005_a4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynamic(optimiseur_a4a,modele_a4a,X_train_a4a,y_train_a4a,X_test_a4a,y_test_a4a,init_a4a,L_a4a,max_iters,criterion,target,\"dynamic_0005_a4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal(optimiseur_a4a,modele_a4a,X_train_a4a,y_train_a4a,X_test_a4a,y_test_a4a,init_a4a,L_a4a,max_iters,criterion,target,\"optimal_0005_a4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact(optimiseur_a4a,modele_a4a,X_train_a4a,y_train_a4a,X_test_a4a,y_test_a4a,init_a4a,L_a4a,max_iters,criterion,target,\"exact_0005_a4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodic(optimiseur_a4a,modele_a4a,X_train_a4a,y_train_a4a,X_test_a4a,y_test_a4a,init_a4a,L_a4a,max_iters,criterion,target,\"periodic_0005_a4a\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
